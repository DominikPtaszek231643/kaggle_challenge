{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T13:02:50.259831Z",
     "start_time": "2024-04-15T13:02:50.244054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(   PassengerId  Survived  Pclass  \\\n 0            1         0       3   \n 1            2         1       1   \n 2            3         1       3   \n 3            4         1       1   \n 4            5         0       3   \n \n                                                 Name     Sex   Age  SibSp  \\\n 0                            Braund, Mr. Owen Harris    male  22.0      1   \n 1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n 2                             Heikkinen, Miss. Laina  female  26.0      0   \n 3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n 4                           Allen, Mr. William Henry    male  35.0      0   \n \n    Parch            Ticket     Fare Cabin Embarked  \n 0      0         A/5 21171   7.2500   NaN        S  \n 1      0          PC 17599  71.2833   C85        C  \n 2      0  STON/O2. 3101282   7.9250   NaN        S  \n 3      0            113803  53.1000  C123        S  \n 4      0            373450   8.0500   NaN        S  ,\n PassengerId      0\n Survived         0\n Pclass           0\n Name             0\n Sex              0\n Age            177\n SibSp            0\n Parch            0\n Ticket           0\n Fare             0\n Cabin          687\n Embarked         2\n dtype: int64,\n        PassengerId    Survived      Pclass         Age       SibSp  \\\n count   891.000000  891.000000  891.000000  714.000000  891.000000   \n mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n std     257.353842    0.486592    0.836071   14.526497    1.102743   \n min       1.000000    0.000000    1.000000    0.420000    0.000000   \n 25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n 50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n 75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n max     891.000000    1.000000    3.000000   80.000000    8.000000   \n \n             Parch        Fare  \n count  891.000000  891.000000  \n mean     0.381594   32.204208  \n std      0.806057   49.693429  \n min      0.000000    0.000000  \n 25%      0.000000    7.910400  \n 50%      0.000000   14.454200  \n 75%      0.000000   31.000000  \n max      6.000000  512.329200  )"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# display the first few rows\n",
    "train_data_head = train_data.head()\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = train_data.isnull().sum()\n",
    "\n",
    "# Get summary\n",
    "summary_statistics = train_data.describe()\n",
    "\n",
    "train_data_head, missing_values, summary_statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spyro\\AppData\\Local\\Temp\\ipykernel_1880\\67026449.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['Age'].fillna(train_data['Age'].median(), inplace=True)\n",
      "C:\\Users\\spyro\\AppData\\Local\\Temp\\ipykernel_1880\\67026449.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['Embarked'].fillna(most_common_embarked, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(   PassengerId  Survived  Pclass  \\\n 0            1         0       3   \n 1            2         1       1   \n 2            3         1       3   \n 3            4         1       1   \n 4            5         0       3   \n \n                                                 Name  Sex   Age  SibSp  Parch  \\\n 0                            Braund, Mr. Owen Harris    1  22.0      1      0   \n 1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0  38.0      1      0   \n 2                             Heikkinen, Miss. Laina    0  26.0      0      0   \n 3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0  35.0      1      0   \n 4                           Allen, Mr. William Henry    1  35.0      0      0   \n \n              Ticket     Fare  Embarked  \n 0         A/5 21171   7.2500         2  \n 1          PC 17599  71.2833         0  \n 2  STON/O2. 3101282   7.9250         2  \n 3            113803  53.1000         2  \n 4            373450   8.0500         2  ,\n        PassengerId    Survived      Pclass         Sex         Age  \\\n count   891.000000  891.000000  891.000000  891.000000  891.000000   \n mean    446.000000    0.383838    2.308642    0.647587   29.361582   \n std     257.353842    0.486592    0.836071    0.477990   13.019697   \n min       1.000000    0.000000    1.000000    0.000000    0.420000   \n 25%     223.500000    0.000000    2.000000    0.000000   22.000000   \n 50%     446.000000    0.000000    3.000000    1.000000   28.000000   \n 75%     668.500000    1.000000    3.000000    1.000000   35.000000   \n max     891.000000    1.000000    3.000000    1.000000   80.000000   \n \n             SibSp       Parch        Fare    Embarked  \n count  891.000000  891.000000  891.000000  891.000000  \n mean     0.523008    0.381594   32.204208    1.536476  \n std      1.102743    0.806057   49.693429    0.791503  \n min      0.000000    0.000000    0.000000    0.000000  \n 25%      0.000000    0.000000    7.910400    1.000000  \n 50%      0.000000    0.000000   14.454200    2.000000  \n 75%      1.000000    0.000000   31.000000    2.000000  \n max      8.000000    6.000000  512.329200    2.000000  )"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fill missing with the median age\n",
    "train_data['Age'].fillna(train_data['Age'].median(), inplace=True)\n",
    "\n",
    "# Fill missing 'Embarked' with the mode\n",
    "most_common_embarked = train_data['Embarked'].mode()[0]\n",
    "train_data['Embarked'].fillna(most_common_embarked, inplace=True)\n",
    "\n",
    "# Drop Cabin column due to a high number of missing values\n",
    "train_data.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['Sex'] = label_encoder.fit_transform(train_data['Sex'])\n",
    "train_data['Embarked'] = label_encoder.fit_transform(train_data['Embarked'])\n",
    "\n",
    "# Check the data\n",
    "cleaned_data_head = train_data.head()\n",
    "cleaned_data_summary = train_data.describe()\n",
    "\n",
    "cleaned_data_head, cleaned_data_summary\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T13:04:19.633452Z",
     "start_time": "2024-04-15T13:04:16.869994Z"
    }
   },
   "id": "89e306923f886c5e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f06bf180c1b36516"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.7934797596769428"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Create new feature FamilySize\n",
    "train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\n",
    "\n",
    "# Select features\n",
    "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'FamilySize']\n",
    "X = train_data[features]\n",
    "y = train_data['Survived']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train\n",
    "logistic_model = LogisticRegression(max_iter=400)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate \n",
    "cv_scores = cross_val_score(logistic_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Mean accuracy \n",
    "cv_mean_accuracy = np.mean(cv_scores)\n",
    "\n",
    "cv_mean_accuracy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T13:07:27.656492Z",
     "start_time": "2024-04-15T13:07:20.214135Z"
    }
   },
   "id": "18ade5dc24a3e38b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "91c5860640a46e96"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spyro\\AppData\\Local\\Temp\\ipykernel_1880\\3733425291.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined['Age'].fillna(combined['Age'].median(), inplace=True)\n",
      "C:\\Users\\spyro\\AppData\\Local\\Temp\\ipykernel_1880\\3733425291.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined['Fare'].fillna(combined['Fare'].median(), inplace=True)\n",
      "C:\\Users\\spyro\\AppData\\Local\\Temp\\ipykernel_1880\\3733425291.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined['Embarked'].fillna(most_common_embarked, inplace=True)\n",
      "C:\\Users\\spyro\\AppData\\Local\\Temp\\ipykernel_1880\\3733425291.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data.drop('Cabin', axis=1, inplace=True)\n",
      "C:\\Users\\spyro\\AppData\\Local\\Temp\\ipykernel_1880\\3733425291.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data.drop('Cabin', axis=1, inplace=True)\n",
      "C:\\Users\\spyro\\AppData\\Local\\Temp\\ipykernel_1880\\3733425291.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\n",
      "C:\\Users\\spyro\\AppData\\Local\\Temp\\ipykernel_1880\\3733425291.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Combine train and test data temporarily to ensure consistent feature encoding\n",
    "combined = pd.concat([train_data, test_data])\n",
    "\n",
    "# Fill missing 'Age' and 'Fare' with the median from the combined data\n",
    "combined['Age'].fillna(combined['Age'].median(), inplace=True)\n",
    "combined['Fare'].fillna(combined['Fare'].median(), inplace=True)\n",
    "\n",
    "# Fill missing 'Embarked' with the mode from the combined data\n",
    "most_common_embarked = combined['Embarked'].mode()[0]\n",
    "combined['Embarked'].fillna(most_common_embarked, inplace=True)\n",
    "\n",
    "# Encode categorical variables 'Sex' and 'Embarked'\n",
    "label_encoder_sex = LabelEncoder()\n",
    "label_encoder_embarked = LabelEncoder()\n",
    "combined['Sex'] = label_encoder_sex.fit_transform(combined['Sex'])\n",
    "combined['Embarked'] = label_encoder_embarked.fit_transform(combined['Embarked'])\n",
    "\n",
    "# Separate the combined data back into train and test datasets\n",
    "train_data = combined.iloc[:len(train_data)]\n",
    "test_data = combined.iloc[len(train_data):]\n",
    "\n",
    "# Drop 'Cabin' column as it has too many missing values\n",
    "train_data.drop('Cabin', axis=1, inplace=True)\n",
    "test_data.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "# Feature engineering: Create new feature 'FamilySize'\n",
    "train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\n",
    "test_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\n",
    "\n",
    "# Prepare features for model training and prediction\n",
    "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'FamilySize']\n",
    "X_train = train_data[features]\n",
    "y_train = train_data['Survived']\n",
    "X_test = test_data[features]\n",
    "\n",
    "# Train a Bagging Classifier\n",
    "model = BaggingClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T15:22:03.178308Z",
     "start_time": "2024-04-15T15:22:02.927283Z"
    }
   },
   "id": "920427e4d138a524",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_data['PassengerId'],\n",
    "    'Survived': predictions\n",
    "})\n",
    "\n",
    "# Save the submission file ensuring the header is included and the index is not saved\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T15:49:12.109850Z",
     "start_time": "2024-04-15T15:49:12.104051Z"
    }
   },
   "id": "c50ffbf7222d6058",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion for the Approach Using Logistic Regression with Median Imputation\n",
    "\n",
    "This Python script demonstrates an organized approach to preprocessing and analyzing the Titanic dataset to estimate survival rates using the Logistic Regression classifier. This technique is highlighted by several critical processes, which contribute to data preparation and predictive model building:\n",
    "\n",
    "#### Data Preprocessing:\n",
    "\n",
    "- **One-Hot Encoding**: Categorical variables such as 'Sex' and 'Embarked' are transformed into a format suitable for logistic regression models using one-hot encoding. This is crucial because logistic regression, like most machine learning algorithms, requires numerical input.\n",
    "  \n",
    "- **Dropping Columns**: The script removes columns like 'Ticket', 'Cabin', 'Name', 'SibSp', and 'Parch'. These columns are excluded because they either possess a high cardinality, contain numerous unique values, or are sparsely filled, which could complicate the model without significantly enhancing its predictive accuracy.\n",
    "\n",
    "#### Handling Missing Values:\n",
    "\n",
    "- **Median Imputation**: Missing values in the dataset are filled using the medians of respective columns. Opting for the median over the mean is advantageous because it is less sensitive to outliers, which is particularly pertinent given the skewness present in variables like 'Fare'.\n",
    "\n",
    "#### Modeling with Logistic Regression:\n",
    "\n",
    "- **Model Training**: Logistic Regression is utilized due to its effectiveness in binary classification tasks and its interpretability. It's well-suited for a dataset with binary outcomes and provides a robust baseline for comparison with more complex models.\n",
    "  \n",
    "- **Model Evaluation**: The model's performance is assessed using cross-validation within the training dataset, providing a reliable estimate of its accuracy.\n",
    "\n",
    "#### Prediction and Submission:\n",
    "\n",
    "- **Prediction on Test Data**: The trained model is used to predict survival on an unseen test dataset. These predictions are formatted in accordance with the competition's submission requirements.\n",
    "\n",
    "- **Submission File Creation**: Predictions are compiled into a CSV file, tailored for submission, showcasing the model's practical application in generating actionable insights.\n",
    "\n",
    "#### Overall Evaluation\n",
    "\n",
    "This method systematically addresses the challenges of categorical and missing data, utilizes a fundamental yet powerful classification algorithm, and ensures that the outcomes are ready for practical application. The implementation of Logistic Regression, coupled with meticulous preprocessing and evaluation phases, establishes a solid foundation for achieving high predictive accuracy. This structured strategy not only aims for high accuracy but also minimizes the risk of overfitting through prudent feature management. The result is a model that is well-prepared to make accurate predictions on the Titanic survival dataset, making it suitable for submission to predictive modeling competitions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2301a9082f78867"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
